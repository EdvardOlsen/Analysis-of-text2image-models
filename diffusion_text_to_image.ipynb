{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-5J5nCskJks"
      },
      "source": [
        "## installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEtVCkGLj6-z"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/CompVis/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip3 install transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "!mkdir -p models/ldm/text2img-large/\n",
        "!wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UP-ldA9k9AW"
      },
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-t7vC8knX4l"
      },
      "outputs": [],
      "source": [
        "cd /content/latent-diffusion/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HAaHXt8Knq14"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('/content/taming-transformers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLe6HTPzk_FH"
      },
      "outputs": [],
      "source": [
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\")  # TODO: Optionally download from same location as ckpt and chnage this logic\n",
        "model = load_model_from_config(config, \"../models/ldm/text2img-large/model.ckpt\")  # TODO: check path\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "sampler = DDIMSampler(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIL_0DwikQWN"
      },
      "source": [
        "## process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kZRaNpMYpuyi"
      },
      "outputs": [],
      "source": [
        "prompt = \"3 red apples and 2 bananas on a silver plate\" # the prompt to render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rJfT__OSlDa2"
      },
      "outputs": [],
      "source": [
        "outdir = 'outputs' # dir to write results to\n",
        "ddim_steps = 200 # number of ddim sampling steps\n",
        "ddim_eta = 0 # ddim eta (eta=0.0 corresponds to deterministic sampling\n",
        "n_iter = 1 # sample this often\n",
        "H,W = 256, 256 # \"image height, width in pixel space\n",
        "scale = 5.0 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
        "n_samples = 2 # how many samples to produce for the given prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voj3gAWgj_6J"
      },
      "outputs": [],
      "source": [
        "os.makedirs(outdir, exist_ok=True)\n",
        "outpath = outdir\n",
        "\n",
        "sample_path = os.path.join(outpath, \"samples\")\n",
        "os.makedirs(sample_path, exist_ok=True)\n",
        "base_count = len(os.listdir(sample_path))\n",
        "\n",
        "all_samples=list()\n",
        "with torch.no_grad():\n",
        "    with model.ema_scope():\n",
        "        uc = None\n",
        "        if scale != 1.0:\n",
        "            uc = model.get_learned_conditioning(n_samples * [\"\"])\n",
        "        for n in trange(n_iter, desc=\"Sampling\"):\n",
        "            c = model.get_learned_conditioning(n_samples * [prompt])\n",
        "            shape = [4, H//8, W//8]\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                              conditioning=c,\n",
        "                                              batch_size=n_samples,\n",
        "                                              shape=shape,\n",
        "                                              verbose=False,\n",
        "                                              unconditional_guidance_scale=scale,\n",
        "                                              unconditional_conditioning=uc,\n",
        "                                              eta=ddim_eta)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "            for x_sample in x_samples_ddim:\n",
        "                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
        "                base_count += 1\n",
        "            all_samples.append(x_samples_ddim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0HtspTp4kcIj"
      },
      "outputs": [],
      "source": [
        "# additionally, save as grid\n",
        "grid = torch.stack(all_samples, 0)\n",
        "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "grid = make_grid(grid, nrow=n_samples)\n",
        "\n",
        "# to image\n",
        "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7ua5gA3m3Bn"
      },
      "outputs": [],
      "source": [
        "Image.open(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1Lmo7ZZqILwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "diffusion-text-to-image.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}